---
title: 'EspressoDB: A scientific database for managing high-performance computing workflow'
tags:
  - Python
  - Django
  - High-performance computing
  - Lattice QCD
authors:
  - name: Chia Cheng Chang
    orcid: 0000-0002-3790-309X
    affiliation: "1, 2, 3"
  - name: Christopher Körber
    orcid: 0000-0002-9271-8022
    affiliation: "2, 3"
  - name: André Walker-Loud
    orcid: 0000-0002-4686-3667
    affiliation: "3, 2"
affiliations:
 - name: iTHEMS RIKEN, Wako, Saitama 351-0198
   index: 1
 - name: Department of Physics, University of California, Berkeley, California 94720
   index: 2
 - name: Nuclear Science Division, Lawrence Berkeley National Laboratory, Berkeley, California 94720
   index: 3

date: 3 December 2019
bibliography: paper.bib

---

# Summary

Leadership computing facilities around the world support cutting-edge scientific research across a broad spectrum of disciplines including understanding climate change [@Kurth_2018], combating opioid addiction [@Joubert:2018:AOE:3291656.3291732], or simulating the decay of a neutron [@Berkowitz:2018gqe].
While the increase in computational power has allowed scientists to better evaluate the
underlying model, the size of these computational projects have grown to a point where
a framework is desired to facilitate managing the workflow.
A typical scientific computing workflow includes:

1. Defining all input parameters for every step of the computation;
2. Defining dependencies of computational tasks;
3. Storing some of the output data to tape;
4. Post-processing these data files;
5. Performing data analysis on output.

[``EspressoDB``](https://github.com/callat-qcd/espressodb/) is a programmatic object-relational data management framework implemented in Python and based on the [``Django`` web framework](https://www.djangoproject.com).
``EspressoDB`` was developed to streamline data management workflows, centralize and guarantee data integrity, while providing domain flexibility and ease of use.
The [*Features* section](#features) summarizes ``EspressoDB``.

[``LatteDB``](https://github.com/callat-qcd/lattedb/), a version of ``EspressoDB`` <span style="color:red">[ANDRE: is it a version?  this does not seem like the correct description]</span> that is specialized to contain table definitions for Lattice QCD calculations, is currently being used to manage the workflow of the 2019 INCITE project titled *The Proton's Structure and the Search for New Physics* [@incite:2019]
and it will be used for the 2020 INCITE project titled *The Structure and Interactions of Nucleons from the Standard Model* [@incite:2020].
The corresponding website generated by ``LatteDB`` can be found at [https://ithems.lbl.gov/lattedb/](https://ithems.lbl.gov/lattedb/).
A precursor to ``EspressoDB`` and ``LatteDB`` was used to support a series of Lattice QCD projects
[@Nicholson:2018mwc; @Chang:2018uxx].
The framework provided by ``EspressoDB`` aims to support the ever increasing complexity of workflows of scientific computing at leadership computing facilities, with the goal of reducing the amount of human time required simply to manage the jobs, thus giving scientists more time to focus on science.
The [*Use case* section](#use-case) presents how ``LatteDB`` is integrated in the INCITE project.

# Features

``EspressoDB`` <span style="color:blue">is written in Python's ``Django`` Object-Relational Mapping (ORM) framework.
As a result, instead of having a (file based) table system, tables correspond to Python classes and objects correspond to rows in this table.
Columns of such tables correspond to attributes of the objects.
Thus it is possible to filter for objects by their attributes or generate summary tables (``pandas.DataFrame``) within one line of code.
Furthermore, using an ORM allows to have the same interface independent of the backend.
It is possible to store data in a file based `SQLite` solution, or use more scalable options like `MySQL` or `Postgresql`.

``Django`` is part of many open-source projects and thus comes with extensive documentation.
Additionally, ``Django`` is scalable, comes with reliable tests and vast community support which manifests in the fact that it is commonly  used in large scale projects (BitBucket, Instagram, Mozilla, NASA and many more).
One guiding principle of ``EspressoDB`` is to not "re-invent the wheel" but instead leverage the support coming from ``Django``.
As a result, one can easily incorporate many of ``Django``'s extensions and find solutions to technical questions online.

Data integrity is important to scientific projects and becomes more challenging the larger the project.
In general, a SQL framework type-checks data before writing to the database and controls dependencies and relations between different tables to ensure internal consistency.
 ``EspressoDB`` implements an abstract ``Base`` class which allows additional user-defined constraints not supported by SQL (*e.g.* unique constraint where the data type is a list of Foreign Keys).
Once the user has specified a set of conditions entries have to fulfill for each table, ``EspressoDB`` runs these cross checks for new data before storing it.

Another aspect of ``EspressoDB`` is to support collaborative and open-data oriented projects.
For this, ``Django``'s web hosting component is utilized and extended.
In addition to providing a centralized data platform, it is possible to spawn[^1] customized web pages which can be viewed, on a local machine only, a local network or the world wide web.
<span style="color:blue">With the default setting, utilizing the ``Base`` class, ``EspressoDB`` spawns:</span>

* Documentation views of implemented tables;
* A project wide notification system;
* Project specific Python interface guidelines which help writing scripts to populate the database;
* Admin pages for interacting with data in a GUI.

Further views can be implemented to interact with data and utilize existing Python libraries for summarizing and visualizing information.
<span style="color:blue">This allows users to create visual progress updates on the fly and to integrate the database information to the data-processing workflow, significantly reducing the human overhead required due to improved automation.</span>

More details, usage instructions and examples are documented at [espressodb.readthedocs.io](https://espressodb.readthedocs.io).

[^1]: Depending on the configuration, it is possible to provide selected access for multiple users on different levels.


# Use case

For an explicit use case, we describe the use of ``LatteDB`` by the CalLat Collaboration ([https://a51.lbl.gov/~callat/webhome/](https://a51.lbl.gov/~callat/webhome/)) in their computations on Summit at the Oak Ridge Leadership Computing Facility (OLCF) through DOE INCITE Allocations [@incite:2019; @incite:2020].  Lattice QCD is one of the main applications awarded time at leadership computing facilities each year through the competitive DOE [INCITE](https://www.doeleadershipcomputing.org) and [ALCC](https://science.osti.gov/ascr/Facilities/Accessing-ASCR-Facilities/ALCC) Allocations.  

Lattice QCD (LQCD) is an inherently a stochastic method of simulating Quantum Chromodynamics (QCD) which is the fundamental theory of nuclear strong interactions, which is responsible for confining quarks into protons and neutrons and ultimately, for binding these nucleons into the atomic nuclei we observe in nature.
The application of LQCD to forefront research applications in nuclear physics is an exascale challenge problem, see for example the recent review [@Drischler:2019xuo].  One of the main reasons these calculations are so expensive is that when LQCD is applied to one or more nucleons, an exponentially bad signal-to-noise problem must be overcome.  While the optimal strategy for overcoming this challenge is not yet known, one thing common to all methods is the need for an exponentially large amount of statistics.
As such, these LQCD computations require the completion of hundreds of thousands to millions of independent sub-calculations (or tasks), with chained dependencies, in order to complete a single calculation.  These chained tasks write large amounts of temporary files to the scratch file system which are used as input for subsequent files, often with multiple input files required for the later computations.
Several such calculations (each with the hundreds-of-thousands to millions of independent tasks) must be performed in order to extrapolate the results to the limit of zero discretization (the continuum limit), infinite volume and to the physical values of the small number of input parameters, which are _a priori_ unknown.






> `@walkloud`, `@cchang5`: We need more verbosity here --- below you can find some ideas of what might fit.

* ``LatteDB`` is an example project which describes Lattice Quantum Chromo Dynamics computations and utilizes ``EspressoDB``.
* Computations are run on (several) remote machines.
* Such computations generate ~ 200TB distributed over ~ 400k files.
* Depending on machine availability and runtime information, it is not guaranteed that (a) files are generated in a predictable pattern, (b) all generated files are generated correctly (file size)
* To obtain physical results, these files need to be combined depending on their meta information.
* ``LatteDB`` is used to monitor the status, generate new jobs depending on the progress (missing files) and stored finished objects to tape.

![Example table view of file status with column specific filters and dynamic progress bar.](doc-src/_static/lattedb-example.png)

# Acknowledgements

The authors thank Even Berkowitz, Arjun Gambhir, Ben Hörz,  Kenneth McElvain and Enrico Rinaldi for useful insights and discussions which helped in creating ``EspressoDB`` and ``LatteDB``.
C.K. gratefully acknowledges funding through the Alexander von Humboldt Foundation through a Feodor Lynen Research Fellowship.
The work of A.W-L. was supported in part by the U.S. Department of Energy Exascale Computing Project.

# References
